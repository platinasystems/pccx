version: '3.4'

configs:
  alertmanager:
    file: ./config/alertmanager.yml
  key-manager-supervisor:
    file: config/supervisor/key-manager.conf
  mailer:
    file: config/mailer.yml
  monitor:
    file: config/monitor.yml
  monitor-supervisor:
    file: config/supervisor/monitor.conf
  pccserver-engine:
    file: ./config/pccserver-engine.sh
  pccserver-supervisor:
    file: config/supervisor/pccserver.conf
  postgres:
    file: ./config/postgresql.conf
  prometheus:
    file: ./config/prometheus.yml
  phone-home-supervisor:
    file: config/supervisor/phone-home.conf
  ssh-id:
    file: secret/id_rsa.pub
  user-management:
    file: config/user-management.yml
  user-management-supervisor:
    file: config/supervisor/user-management.conf

secrets:
  env:
    file: .env
  cert:
    file: secret/cert.pem
  gateway:
    file: config/gateway.env
  kafka:
    file: secret/kafka.env
  key:
    file: secret/key.pem
  key-manager:
    file: config/key-manager.env
  maas:
    file: config/maas.env
  mailer:
    file: config/mailer.env
  minio:
    file: secret/minio.env
  monitor:
    file: config/monitor.env
  pcc-ui:
    file: config/pcc-ui.env
  pccserver:
    file: config/pccserver.env
  pgpass:
    file: secret/.pgpass
  phone-home:
    file: config/phone-home.env
  phone-home-storage:
    file: secret/phone-home-storage.yml
  platina-executor:
    file: config/platina-executor.env
  platina-monitor:
    file: config/platina-monitor.env
  security:
    file: config/security.env
  ssh-id:
    file: secret/id_rsa
  user-management:
    file: config/user-management.env

x-custom:
- &public-ssh-id
  source: ssh-id
  target: /root/.ssh/id_rsa_ansible.pub
- &secret-ssh-id
  source: ssh-id
  target: /root/.ssh/id_rsa_ansible

services:
  tlsx:
    container_name: tlsx
    image: ${TLSX_IMAGE}
    restart: ${RESTART}
    ports:
    - ${TLSX_PORT}:8003
    environment:
      TLSX_CERT_FILE: /run/secrets/cert
      TLSX_KEY_FILE: /run/secrets/key
      TLSX_SUBSCRIBERS_FILE: /var/run/ko/subscribers
    secrets: [ cert, key ]
    volumes:
    - ./data/tlsx:/var/run/ko
    command:
    - -verbose=${TLSX_VERBOSE}
    - start
    - exchange
    - ${TLSX_VPN}

  tap:
    container_name: tap
    depends_on: [ tlsx ]
    image: ${TLSX_IMAGE}
    restart: ${RESTART}
    networks:
      default:
      backend:
        aliases:
        - kafka
        - pccserver
    expose:
    - 3030
    - 8989
    ports:
    - 8081:8081
    - 9092:9092
    cap_add: [ ALL ]
    devices: [ /dev/net/tun ]
    privileged: true
    user: "0:0"
    environment:
      TLSX_CERT_FILE: /run/secrets/cert
      TLSX_KEY_FILE: /run/secrets/key
    secrets: [ cert, key ]
    command:
    - -verbose=${TLSX_VERBOSE}
    - start
    - tap
    - -a
    - ${TLSX_TAP_ADDR}
    - "@tlsx"

  kafka:
    container_name: kafka
    platform: linux/amd64
    image: ${KAFKA_IMAGE}
    restart: ${RESTART}
    network_mode: "service:tap"
    privileged: true
    user: "0:0"
    env_file: [ .env, secret/kafka.env ]
    volumes:
    - ./data/kafka:/data
    - ./config/kafka/log4j.properties:/opt/landoop/kafka/etc/kafka/log4j.properties:ro
    - ./config/kafka/schema-registry.properties:/run/schema-registry/schema-registry.properties:ro
    - ./config/kafka/09-topic-config.conf:/etc/supervisord.d/09-topic-config.conf:ro
    - ./config/kafka/topic-config.sh:/opt/landoop/kafka/bin/topic-config.sh
    - ./patch/zookeeper-3.5.9-patch.jar:/opt/landoop/kafka/share/java/kafka/zookeeper-3.5.9.jar
    - ./patch/zookeeper-3.5.9-patch.jar:/opt/landoop/kafka/share/java/schema-registry/zookeeper-3.5.8.jar
    - ./patch/zookeeper-3.5.9-patch.jar:/opt/landoop/kafka/share/java/landoop-common/zookeeper-3.5.7.jar
    - ./patch/zookeeper-3.5.9-patch.jar:/opt/landoop/connectors/third-party/kafka-connect-s3/zookeeper-3.5.8.jar
    - ./patch/zookeeper-jute-3.5.9.jar:/opt/landoop/kafka/share/java/kafka/zookeeper-jute-3.5.9.jar
    - ./patch/zookeeper-jute-3.5.9.jar:/opt/landoop/kafka/share/java/schema-registry/zookeeper-jute-3.5.8.jar
    - ./patch/zookeeper-jute-3.5.9.jar:/opt/landoop/kafka/share/java/landoop-common/zookeeper-jute-3.5.7.jar
    - ./patch/zookeeper-jute-3.5.9.jar:/opt/landoop/connectors/third-party/kafka-connect-s3/zookeeper-jute-3.5.8.jar

  pccserver:
    container_name: pccserver
    depends_on: [ kafka, postgres, minio ]
    platform: linux/amd64
    image: ${PCC_IMAGE}
    restart: ${RESTART}
    network_mode: "service:tap"
    privileged: true
    cap_add:
    - SYS_PTRACE
    - NET_ADMIN
    - SYS_ADMIN
    devices: [ /dev/net/tun ]
    configs:
    - source: pccserver-engine
      target: /home/scripts/microservice/engine.sh
    - source: pccserver-supervisor
      target: /home/conf/pccserver.conf
    - *public-ssh-id
    secrets:
    - env
    - kafka
    - minio
    - pccserver
    - *secret-ssh-id
    volumes:
    - ./config/pccserver.yml:${SERVICE_CONFIG_URI}
    - ./data/pccserver:/srv/pcc
    - ./log/pccserver:/home/logs
    - ops-playbooks:/home/ops
    - orchestration-playbooks:/home/orchestration
    - kubespray2-playbooks:/home/kubespray
    - kubespray-playbooks:/home/turnkey-kubespray
    - pccserver-playbooks:/home/ansible

  redis:
    container_name: redis
    image: ${REDIS_IMAGE}
    networks:
      backend:
        aliases:
        - monitor_redis
    expose: [ 6379 ]
    volumes:
    - ./data/redis:/data

  postgres:
    container_name: postgres
    platform: linux/amd64
    image: ${POSTGRES_IMAGE}
    restart: ${RESTART}
    networks:
      backend:
        aliases:
        - postgres-db
    expose: [ 5432 ]
    env_file: .env
    configs:
    - source: postgres
      target: /etc/postgresql.conf
    secrets: [ env ]
    volumes:
    - ./data/postgres:/var/lib/postgresql/data
    - ./data/postgres-backup:/var/lib/postgresql/backup
    - ./data/postgres-wals:/var/lib/postgresql/wals
    - ./log/postgres:/var/lib/postgresql/logs
    - ./entrypoint/postgres:/docker-entrypoint.sh:ro
    # FIXME - ./entrypoint/postgres-init:/docker-entrypoint-initdb.d:ro
    # FIXME - ./entrypoint/postgres-sql:/docker-entrypoint-sql:ro
    command: postgres -c config_file=/etc/postgresql.conf

  prometheus:
    container_name: prometheus
    image: ${PROMETHEUS_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 9090 ]
    privileged: true
    user: "0:0"
    configs:
    - source: prometheus
      target: /config.yml
    volumes:
    - ./data/prometheus:/prometheus
    - &prometheus-rules ./data/prometheus-rules:/home/prometheus/rules/
    command: >-
      --config.file=/config.yml
      --web.enable-lifecycle
      --storage.tsdb.retention.time=15d

  adminer:
    container_name: adminer
    depends_on: [ postgres ]
    image: ${ADMINER_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 8080 ]
    command: php -S "[::]:8080" -t /var/www/html

  minio:
    container_name: minio
    image: ${MINIO_IMAGE}
    restart: ${RESTART}
    networks:
      backend:
        aliases:
        - storage
    expose: [ 9000, 9001 ]
    environment:
      MINIO_CONFIG_ENV_FILE: "/run/secrets/minio"
    secrets:
    - env
    - minio
#   - source: cert
#     target: /mnt/certs/public.crt
#     uid: ${MINIO_UID}
#     gid: ${MINIO_GID}
#     mode: 0400
#   - source: key
#     target: /mnt/certs/private.key
#     uid: ${MINIO_UID}
#     gid: ${MINIO_GID}
#     mode: 0400
    volumes:
    - ./data/minio:/data
    healthcheck:
      interval: 30s
      timeout: 2s
      test:
      - "CMD"
      - "curl"
      - "-f"
      - "-k"
#     - "--cacert"
#     - "/mnt/certs/public.crt"
      - "http://localhost:9000/minio/health/live"
    command: server --certs-dir=/mnt/certs --console-address ":9001" /data

  cadvisor:
    container_name: cadvisor
    image: ${CADVISOR_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 8080 ]
    labels:
      org.label-schema.group: "monitoring"
    privileged: true
    devices: [ "/dev/kmsg:/dev/kmsg" ]
    volumes:
    - /:/rootfs:ro
    - /sys:/sys:ro
    - /var/lib/docker:/var/lib/docker:ro
    - /etc/machine-id:/etc/machine-id:ro
    - /var/lib/dbus/machine-id:/var/lib/dbus/machine-id:ro
    command:
    - '--docker_only=true'
    - '--housekeeping_interval=15s'
    - '--port=8080'

  node-exporter:
    container_name: node-exporter
    image: ${NODE_EXPORTER_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 9100 ]
    volumes:
    - /proc:/host/proc:ro
    - /sys:/host/sys:ro
    - /:/rootfs:ro
    command:
    - '--path.procfs=/host/proc'
    - '--path.rootfs=/rootfs'
    - '--path.sysfs=/host/sys'
    - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'

  alertmanager:
    container_name: alertmanager
    image: ${ALERTMANAGER_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 9093 ]
    labels:
      org.label-schema.group: "monitoring"
    configs:
    - source: alertmanager
      target: /config.yml
    volumes:
    - ./data/alertmanager:/data
    command:
    - '--config.file=/config.yml'
    - '--storage.path=/data'

  pushgateway:
    container_name: pushgateway
    image: ${PUSHGATEWAY_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 9091 ]
    labels:
      org.label-schema.group: "monitoring"
    command: [ '--web.enable-admin-api' ]

  api-registry:
    container_name: api-registry
    depends_on: [ postgres ]
    platform: linux/amd64
    image: ${APIREGISTRY_IMAGE}
    restart: ${RESTART}
    networks:
      backend:
        aliases:
        - registry
    expose: [ 8761 ]
    env_file: .env
    environment:
      SERVICE_DEBUG_ENABLED: ${APIREGISTRY_DEBUG_ENABLED}
    secrets: [ env ]
    volumes:
    - ./log/api-registry:/home/logs

  gateway:
    container_name: gateway
    platform: linux/amd64
    image: ${GATEWAY_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    ports: [ "443:9999" ]
    expose: [ 9999 ]
    env_file: [ .env, config/gateway.env ]
    secrets:
    - env
    - gateway
    - source: cert
      target: /home/certs/server.crt
      uid: ${GATEWAY_UID}
      gid: ${GATEWAY_GID}
      mode: 0400
    - source: key
      target: /home/certs/server.key
      uid: ${GATEWAY_UID}
      gid: ${GATEWAY_GID}
      mode: 0400
    volumes:
    - ./config/gateway.yml:${SERVICE_CONFIG_URI}
    - ./log/gateway:/home/logs

  mailer:
    container_name: mailer
    depends_on: [ api-registry, postgres ]
    platform: linux/amd64
    image: ${MAILER_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 8787 ]
    cap_add: [ SYS_PTRACE ]
    env_file: [ .env, config/mailer.env ]
    configs:
    - source: mailer
      target: /home/conf/default.yml
    secrets: [ env, mailer ]
    volumes:
    - ./config/mailer.yml:${SERVICE_CONFIG_URI}
    - ./log/mailer:/home/logs

  user-management:
    container_name: user-management
    depends_on: [ postgres, mailer, minio ]
    platform: linux/amd64
    image: ${USERMANAGEMENT_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 8083 ]
    env_file:
    - .env
    - config/user-management.env
    - secret/minio.env
    environment:
      TARGET_BOOT_ARGS: >-
        -Dserver.port=8083
        -Dspring.application.name=user-management
        -Dspring.config.location=${SERVICE_CONFIG_URI}
        -Deureka.client.service-url.defaultZone=${APIREGISTRY_URI}
        -Dspring.config.active.on-profile=docker
        -Dspring.cloud.config.failFast=false
        -Dspring.datasource.username=${USERMANAGEMENT_DB_USER}
        -Dspring.datasource.password=${USERMANAGEMENT_DB_PASSWORD}
        ${JAVA_OPTS}
    configs:
    - source: user-management
      target: /home/conf/default.yml
    - source: user-management-supervisor
      target: /etc/supervisor/conf.d/supervisord.conf
    secrets:
    - env
    - minio
    - user-management
    volumes:
    - ./config/user-management.yml:${SERVICE_CONFIG_URI}
    - ./log/user-management:/home/logs

  security:
    container_name: security
    depends_on: [ postgres ]
    platform: linux/amd64
    image: ${SECURITY_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 8082 ]
    env_file:
    - .env
    - config/security.env
    secrets:
    - env
    - security
    volumes:
    - ./config/security.yml:${SERVICE_CONFIG_URI}
    - ./log/security:/home/logs

  key-manager:
    container_name: key-manager
    depends_on: [ postgres ]
    platform: linux/amd64
    image: ${KEYMANAGER_IMAGE}
    restart: ${RESTART}
    cap_add: [ ALL ]
    privileged: true
    user: "0:0"
    networks: [ backend ]
    expose: [ 8084 ]
    env_file: [ .env, config/key-manager.env ]
    environment:
      TARGET_BOOT_ARGS: >-
        -Dserver.port=8084
        -Dspring.application.name=key-manager
        -Deureka.client.service-url.defaultZone=${APIREGISTRY_URI}
        -Dkey.algorithm=RSA
        -Dkey.size=2048
        -Dspring.application.name=key-manager
        -Dspring.config.location=${SERVICE_CONFIG_URI}
        -Dspring.cloud.config.failFast=false
        -Dspring.datasource.url=jdbc:postgresql://postgres/${KEYMANAGER_DB}
        -Dspring.datasource.driver-class-name=org.postgresql.Driver
        -Dspring.datasource.username=${KEYMANAGER_DB_USER}
        -Dspring.datasource.password=${KEYMANAGER_DB_PASSWORD}
        ${JAVA_OPTS}
    configs:
    - source: key-manager-supervisor
      target: /etc/supervisor/conf.d/supervisord.conf
    - source: ssh-id
      target: /home/keys/system.pub
    secrets:
    - env
    - key-manager
    - source: ssh-id
      target: /home/keys/system.pem
      uid: ${KEYMANAGER_UID}
      gid: ${KEYMANAGER_GID}
      mode: 0400
    volumes:
    - ./config/key-manager.yml:${SERVICE_CONFIG_URI}
    - ./log/key-manager:/home/logs

  monitor:
    container_name: monitor
    depends_on: [ redis, kafka ]
    platform: linux/amd64
    image: ${MONITOR_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 9191 ]
    env_file: [ .env, config/monitor.env ]
    environment:
      TARGET_BOOT_ARGS: >-
        -Dserver.port=9191
        -Dspring.application.name=monitor
        -Deureka.client.service-url.defaultZone=${APIREGISTRY_URI}
        -Dspring.config.location=${SERVICE_CONFIG_URI}
        -Dspring.config.active.on-profile=docker
        -Dspring.cloud.config.failFast=false
        -Dspring.cloud.config.label=${MONITOR_CONFIG_BRANCH}
        ${JAVA_OPTS} -Xmx4096m
    configs:
    - source: monitor
      target: /home/conf/default.yml
    - source: monitor-supervisor
      target: /etc/supervisor/conf.d/supervisord.conf
    secrets: [ env, monitor ]
    volumes:
    - ./config/monitor.yml:${SERVICE_CONFIG_URI}
    - ./data/monitor:/home/parquetdb
    - ./log/monitor:/home/logs

  platina-executor:
    container_name: platina-executor
    depends_on: [ kafka, pccserver ]
    platform: linux/amd64
    image: ${PLATINAEXECUTOR_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 8998 ]
    privileged: true
    env_file: [ .env, config/platina-executor.env ]
    configs:
    - *public-ssh-id
    secrets:
    - env
    - platina-executor
    - *secret-ssh-id
    volumes:
    - ./config/platina-executor.yml:${SERVICE_CONFIG_URI}
    - ./data/pccserver:/srv/pcc
    - ./log/platina-executor:/home/logs
    - /usr/bin/docker:/usr/bin/docker
    - /var/run/docker.sock:/var/run/docker.sock
    - pccserver-playbooks:/home/ansible:ro
    - kubespray2-playbooks:/home/kubespray:ro
    - kubespray-playbooks:/home/turnkey-kubespray:ro
    - ops-playbooks:/home/ops:ro
    - orchestration-playbooks:/home/orchestration

  platina-monitor:
    container_name: platina-monitor
    depends_on:
    - api-registry
    - postgres
    - kafka
    - prometheus
    - alertmanager
    - pushgateway
    - mailer
    platform: linux/amd64
    image: ${PLATINAMONITOR_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 8995 ]
    env_file: [ .env, config/platina-monitor.env ]
    secrets: [ env, platina-monitor ]
    volumes:
    - ./config/platina-monitor.yml:${SERVICE_CONFIG_URI}
    - ./log/platina-monitor:/home/logs
    - *prometheus-rules

  maas:
    container_name: maas
    depends_on: [ api-registry, postgres ]
    platform: linux/amd64
    image: ${MAAS_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 5980 ]
    privileged: true
    cap_add: [ SYS_PTRACE ]
    env_file: [ .env, config/maas.env ]
    configs:
    - *public-ssh-id
    secrets:
    - env
    - maas
    - *secret-ssh-id
    volumes:
    - ./config/maas.yml:${SERVICE_CONFIG_URI}
    - ./data/pccserver:/srv/pcc
    - ./log/maas:/home/logs

  phone-home:
    container_name: phone-home
    depends_on: [ api-registry ]
    platform: linux/amd64
    image: ${PHONEHOME_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 8383, 9000 ]
    cap_add: [ SYS_PTRACE ]
    env_file:
    - .env
    - config/phone-home.env
    - secret/minio.env
    environment:
      TARGET_BOOT_ARGS: >-
        --overrideSystemProperties
        --configProfile docker
        --dbHost postgres
        --dbName ${PHONEHOME_DB}
        --dbUsername ${PHONEHOME_DB_USER}
        --dbPassword ${PHONEHOME_DB_PASSWORD}
        --configUri ${SERVICE_CONFIG_URI}
        --customer ${PHONEHOME_CUSTOMER}
        --services "platina-monitor,key-manager,monitor,registry,maas,node-exporter,mailer,user-management,platina-executor,cadvisor,gateway,pccserver,security"
        --useHostnameAsAddress
        --useHostnameAsName
        --registryAddress api-registry
        --maxRegistrationRetry 5
    configs:
    - source: phone-home-supervisor
      target: /etc/supervisor/conf.d/supervisord.conf
    secrets:
    - env
    - phone-home
    - source: phone-home-storage
      target: /home/conf/storage.yml
    volumes:
    - ./config/phone-home.yml:${SERVICE_CONFIG_URI}
    - ./data/phone-home:/data
    - ./log/phone-home:/home/logs
    - ${PHONEHOME_PUBKEY_FILE}:/root/.gnupg/pubkey.asc:ro

  pcc-ui:
    container_name: pcc-ui
    platform: linux/amd64
    image: ${PCCUI_IMAGE}
    restart: ${RESTART}
    networks: [ backend ]
    expose: [ 443 ]
    env_file: [ .env, config/pcc-ui.env ]
    secrets:
    - env
    - pcc-ui
    - source: cert
      target: /usr/local/apache2/conf/server.crt
      uid: ${PCCUI_UID}
      gid: ${PCCUI_GID}
      mode: 0400
    - source: key
      target: /usr/local/apache2/conf/server.key
      uid: ${PCCUI_UID}
      gid: ${PCCUI_GID}
      mode: 0400
    volumes:
    - ./config/pcc-ui.yml:${SERVICE_CONFIG_URI}
    - ./log/pcc-ui:/usr/local/apache2/logs

networks:
  backend: {}

volumes:
  tlsx:
  kubespray2-playbooks:
  kubespray-playbooks:
  ops-playbooks:
  orchestration-playbooks:
  pccserver-playbooks:
